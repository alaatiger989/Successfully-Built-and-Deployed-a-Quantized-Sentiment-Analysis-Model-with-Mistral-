{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1351cf60-d08e-4013-b26b-71985b787dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mistralai/Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30bd83-bdea-4a9a-8fa0-9751df2545d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445ef2c2-cb4e-4677-aff6-b3c817d39d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\thegh\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3db426e-be72-4496-9358-14df4c340ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082edc4-f008-43fa-a769-647183517f63",
   "metadata": {},
   "source": [
    "# Load the Mistral model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9057f2-d672-4aea-8018-9be5641b5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, load_in_4bit=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, quantization_config=bnb_config, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a66614-c0b1-496f-ac1b-8dc5c2be74ec",
   "metadata": {},
   "source": [
    "# Explicitly set and verify the pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1307ee5-b815-4155-9b97-a0280711c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed1349-efea-49aa-8d4b-1fcc4f6d5927",
   "metadata": {},
   "source": [
    "# Check if the padding token is correctly set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd30f7-f643-4872-b807-eb5c05d004ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a2a9e-6e14-4546-8295-0dfb5dcbabe9",
   "metadata": {},
   "source": [
    "# Apply PEFT to add trainable LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822e942-f781-418a-861a-70e21280c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d534192-27f9-4f33-8c44-bbff64a2d84e",
   "metadata": {},
   "source": [
    "# Load dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2f8dd-9946-410d-933a-42f56914b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "max_length = 128\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9bbfdf-664a-46d6-8144-2bba6ce08cdc",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a3922-2446-4d32-b039-d8b44c3c30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = encoded_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'validation': test_valid['test'],\n",
    "    'test': test_valid['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd9f03-ee11-4e75-b74c-1d5cff78dd83",
   "metadata": {},
   "source": [
    "# Define training arguments without mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fac1fb-ffe3-4ac4-9222-271ba4999ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,  # Set batch size to 1\n",
    "    gradient_accumulation_steps=16,  # Simulate larger batch size\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,  # Disable FP16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea90739-938b-43a7-8b99-56e5df275023",
   "metadata": {},
   "source": [
    "# Define the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aaefff-6483-47cc-b0cb-329c62b54e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078765a-e89b-47e4-9582-89938fa2ab00",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cdb3dd-ea68-4e1f-b403-3b2ffc488090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thegh\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:23<00:00,  7.69s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD], ID: 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thegh\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\thegh\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thegh\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:674: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='6840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 430/6840 3:13:23 < 48:16:22, 0.04 it/s, Epoch 0.19/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 80\u001b[0m\n\u001b[0;32m     71\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     72\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     73\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1886\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1887\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1888\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1890\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\transformers\\trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\transformers\\trainer.py:3243\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m-> 3243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_apex:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\alaa_ai_env\\Lib\\site-packages\\transformers\\training_args.py:2151\u001b[0m, in \u001b[0;36mTrainingArguments.n_gpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2148\u001b[0m     requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   2149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n\u001b[1;32m-> 2151\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mn_gpu\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;124;03m    The number of GPUs used by this process.\u001b[39;00m\n\u001b[0;32m   2155\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[38;5;124;03m        training. For distributed training, it will always be 1.\u001b[39;00m\n\u001b[0;32m   2159\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2160\u001b[0m     requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a944a70-7909-468b-8e82-b07221c0b1cc",
   "metadata": {},
   "source": [
    "# Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d062c18-de2d-40d5-9ce0-6f49a760da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./quantized_mistral_with_lora\")\n",
    "tokenizer.save_pretrained(\"./quantized_mistral_with_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73264842-b9e1-4703-8d4f-728e193275f5",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1d26f-3086-49ed-ae77-301db04d8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(eval_dataset=dataset[\"test\"])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
